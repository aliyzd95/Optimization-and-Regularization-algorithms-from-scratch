# Optimization-and-Regularization-from-scratch
Implementation of optimization and regularization algorithms in deep neural networks from scratch

In this repository, I implemented and investigated different optimaziation algorithms including Adam, Adagrad, Gradient Descent and RMSProp along with L1 and L2 regularization method to classify samples in the cifar dataset.

### Gradient Descent
<img src='https://user-images.githubusercontent.com/55990659/203638831-f4b36417-11c4-4727-b2b3-3f82c55d7558.png' width='50%'>


### Adagrad
<img src='https://user-images.githubusercontent.com/55990659/203638913-5170fb35-77e1-4674-bac3-d07debc27327.png' width='50%'>


### RMSProp
<img src='https://user-images.githubusercontent.com/55990659/203638970-92f01fb7-a61e-48fc-b8a9-7706bb075745.png' width='50%'>


### Adam
<img src='https://user-images.githubusercontent.com/55990659/203639024-75f09748-44b2-44dc-a5c5-a7aa11379c39.png' width='50%'>
